import { HumanMessage, AIMessage, SystemMessage } from '@langchain/core/messages';
import { BaseChatModel } from '@langchain/core/language_models/chat_models';
import { tool } from '@langchain/core/tools';
import { z } from 'zod';
import { ChatGroq } from '@langchain/groq';
import { configManager } from './config';
import { tauriMCPClient, MCPTool } from './tauri-mcp-client';
import { log } from './logger';
import { llmConfigManager, ServiceConfig } from './llm-config-manager';

interface ModelConfig {
  serviceId?: string; // ì„œë¹„ìŠ¤ IDë¡œ ì„¤ì •ì„ ê°€ì ¸ì˜´
  provider?: string;
  model?: string;
  temperature?: number;
  maxTokens?: number;
  apiKey?: string;
}

export interface StreamableMessage {
  id: string;
  content: string;
  role: 'user' | 'assistant' | 'system';
  thinking?: string;
  isStreaming?: boolean;
  attachments?: { name: string; content: string; }[];
}

export type ChatMode = 'chat' | 'agent';

export class AIService {
  private model?: BaseChatModel;
  private modelConfig: ModelConfig;
  private serviceConfig: ServiceConfig;

  constructor(config: Partial<ModelConfig> = {}) {
    // ì„œë¹„ìŠ¤ IDê°€ ì œê³µëœ ê²½ìš° í•´ë‹¹ ì„¤ì •ì„ ì‚¬ìš©
    if (config.serviceId) {
      this.serviceConfig = llmConfigManager.getServiceConfig(config.serviceId) || llmConfigManager.getDefaultServiceConfig();
    } else {
      this.serviceConfig = llmConfigManager.getDefaultServiceConfig();
    }

    // ê°œë³„ ì„¤ì •ì´ ì œê³µëœ ê²½ìš° ë®ì–´ì“°ê¸°
    this.modelConfig = {
      serviceId: config.serviceId,
      provider: config.provider || this.serviceConfig.provider,
      model: config.model || this.serviceConfig.model,
      temperature: config.temperature ?? this.serviceConfig.temperature,
      maxTokens: config.maxTokens ?? this.serviceConfig.maxTokens,
      apiKey: config.apiKey || this.getApiKeyForProvider(config.provider || this.serviceConfig.provider),
    };

    if (!this.modelConfig.apiKey) {
      throw new Error(`API key not found for provider ${this.modelConfig.provider}. Please set the appropriate environment variable.`);
    }
  }

  private getApiKeyForProvider(providerId: string): string | undefined {
    const provider = llmConfigManager.getProvider(providerId);
    if (!provider) return undefined;
    
    // í™˜ê²½ë³€ìˆ˜ì—ì„œ API í‚¤ ê°€ì ¸ì˜¤ê¸°
    const envVar = provider.apiKeyEnvVar;
    return process.env[envVar] || configManager.getGroqApiKey() || undefined;
  }

  async initializeModel(): Promise<void> {
    try {
      // í˜„ì¬ëŠ” Groqë§Œ ì§€ì›
      if (this.modelConfig.provider === 'groq') {
        this.model = new ChatGroq({
          apiKey: this.modelConfig.apiKey!,
          model: this.modelConfig.model!,
          temperature: this.modelConfig.temperature,
          maxTokens: this.modelConfig.maxTokens,
        });
      } else {
        throw new Error(`Unsupported provider: ${this.modelConfig.provider}`);
      }
      
      log.info(`Initialized model: ${this.modelConfig.provider}:${this.modelConfig.model}`, 'AIService');
    } catch (error) {
      log.error('Failed to initialize model', 'AIService', error instanceof Error ? error : new Error(String(error)));
      throw error;
    }
  }

  private async createMCPTools(availableTools: MCPTool[]): Promise<Array<any>> {
    const tools = [];
    
    for (const mcpTool of availableTools) {
      try {
        // MCP ë„êµ¬ì˜ ìŠ¤í‚¤ë§ˆë¥¼ Zod ìŠ¤í‚¤ë§ˆë¡œ ë³€í™˜
        const zodSchema = this.convertMCPSchemaToZod(mcpTool.input_schema);
        
        const langchainTool = tool(
          async (input: Record<string, unknown>) => {
            return await this.executeMCPTool(mcpTool.name, input);
          },
          {
            name: mcpTool.name,
            description: mcpTool.description,
            schema: zodSchema,
          }
        );
        
        tools.push(langchainTool);
        log.debug(`Created Langchain tool for: ${mcpTool.name}`, 'AIService');
      } catch (error) {
        log.error(`Failed to create tool for ${mcpTool.name}`, 'AIService', error instanceof Error ? error : new Error(String(error)));
      }
    }
    
    return tools;
  }

  private convertMCPSchemaToZod(schema: any): z.ZodType<any> {
    if (!schema || !schema.properties) {
      return z.object({});
    }

    const zodObj: Record<string, z.ZodType<any>> = {};
    
    for (const [key, value] of Object.entries(schema.properties)) {
      const prop = value as any;
      
      switch (prop.type) {
        case 'string':
          zodObj[key] = z.string().describe(prop.description || '');
          break;
        case 'number':
          zodObj[key] = z.number().describe(prop.description || '');
          break;
        case 'boolean':
          zodObj[key] = z.boolean().describe(prop.description || '');
          break;
        case 'array':
          zodObj[key] = z.array(z.any()).describe(prop.description || '');
          break;
        default:
          zodObj[key] = z.any().describe(prop.description || '');
      }
      
      // í•„ìˆ˜ í•„ë“œê°€ ì•„ë‹Œ ê²½ìš° optionalë¡œ ì„¤ì •
      if (!schema.required || !schema.required.includes(key)) {
        zodObj[key] = zodObj[key].optional();
      }
    }
    
    return z.object(zodObj);
  }

  private async executeMCPTool(toolName: string, arguments_: Record<string, unknown>): Promise<string> {
    try {
      const connectedServers = await tauriMCPClient.getConnectedServers();
      
      for (const serverName of connectedServers) {
        const serverTools = await tauriMCPClient.listTools(serverName);
        const serverTool = serverTools.find(t => t.name === toolName);
        
        if (serverTool) {
          log.info(`ğŸ”§ Executing MCP tool '${toolName}' on server '${serverName}'`, 'AIService');
          const result = await tauriMCPClient.callTool(serverName, toolName, arguments_);
          return JSON.stringify(result, null, 2);
        }
      }
      
      throw new Error(`Tool '${toolName}' not found on any connected server`);
    } catch (error) {
      log.error('MCP tool execution error', 'AIService', error instanceof Error ? error : new Error(String(error)));
      throw error;
    }
  }

  async *streamChat(
    messages: StreamableMessage[],
    systemPrompt?: string,
    availableTools?: MCPTool[],
    mode: ChatMode = 'chat'
  ): AsyncGenerator<string, void, unknown> {
    try {
      await this.initializeModel();
      
      if (mode === 'agent' && availableTools && availableTools.length > 0) {
        yield* this.streamAgentMode(messages, systemPrompt, availableTools);
      } else {
        yield* this.streamChatMode(messages, systemPrompt, availableTools);
      }
    } catch (error) {
      log.error('AI Service error', 'AIService', error instanceof Error ? error : new Error(String(error)));
      yield `Error: ${error instanceof Error ? error.message : 'Unknown error'}`;
    }
  }

  private async *streamChatMode(
    messages: StreamableMessage[],
    systemPrompt?: string,
    availableTools?: MCPTool[]
  ): AsyncGenerator<string, void, unknown> {
    if (!this.model) {
      throw new Error('Model not initialized');
    }

    const langchainMessages = [];

    if (systemPrompt) {
      let enhancedSystemPrompt = systemPrompt;
      
      if (availableTools && availableTools.length > 0) {
        enhancedSystemPrompt += `\n\nYou have access to the following tools:\n`;
        for (const tool of availableTools) {
          enhancedSystemPrompt += `- ${tool.name}: ${tool.description}\n`;
          if (tool.input_schema) {
            enhancedSystemPrompt += `  Input schema: ${JSON.stringify(tool.input_schema, null, 2)}\n`;
          }
        }
        enhancedSystemPrompt += `\nTo use a tool, respond with a JSON object in this format:
{
  "tool_call": {
    "name": "tool_name",
    "arguments": {
      "parameter": "value"
    }
  },
  "explanation": "Why you're using this tool"
}

After using a tool, you will receive the result and can continue the conversation.`;
      }
      
      langchainMessages.push(new SystemMessage(enhancedSystemPrompt));
    }

    for (const msg of messages) {
      if (msg.role === 'user') {
        langchainMessages.push(new HumanMessage(msg.content));
      } else if (msg.role === 'assistant') {
        langchainMessages.push(new AIMessage(msg.content));
      }
    }
    
    const stream = await this.model.stream(langchainMessages, {
      configurable: {
        apiKey: this.modelConfig.apiKey,
        model: this.modelConfig.model,
      },
    });
    
    let accumulatedContent = '';
    
    for await (const chunk of stream) {
      if (chunk.content) {
        const content = chunk.content as string;
        accumulatedContent += content;
        yield content;
      }
    }
    
    // ë„êµ¬ í˜¸ì¶œ ì²˜ë¦¬ (Chat ëª¨ë“œì—ì„œ)
    if (availableTools && availableTools.length > 0) {
      const toolCallMatch = accumulatedContent.match(/\{[\s\S]*"tool_call"[\s\S]*\}/);
      if (toolCallMatch) {
        try {
          const toolCallData = JSON.parse(toolCallMatch[0]);
          if (toolCallData.tool_call) {
            yield '\n\nğŸ”§ Using tool: ' + toolCallData.tool_call.name + '\n';
            
            const result = await this.executeMCPTool(toolCallData.tool_call.name, toolCallData.tool_call.arguments);
            yield 'ğŸ“‹ Tool result: ' + result + '\n\n';
          }
        } catch (error) {
          log.error('Error parsing tool call', 'AIService', error instanceof Error ? error : new Error(String(error)));
        }
      }
    }
  }

  private async *streamAgentMode(
    messages: StreamableMessage[],
    systemPrompt?: string,
    availableTools?: MCPTool[]
  ): AsyncGenerator<string, void, unknown> {
    if (!availableTools || availableTools.length === 0) {
      yield* this.streamChatMode(messages, systemPrompt);
      return;
    }

    if (!this.model) {
      throw new Error('Model not initialized');
    }

    try {
      // MCP ë„êµ¬ë¥¼ Langchain ë„êµ¬ë¡œ ë³€í™˜
      const tools = await this.createMCPTools(availableTools);
      
      if (tools.length === 0) {
        yield 'No valid tools available for agent mode. Falling back to chat mode.\n\n';
        yield* this.streamChatMode(messages, systemPrompt, availableTools);
        return;
      }

      // ëª¨ë¸ì— ë„êµ¬ ë°”ì¸ë”©
      if (!('bindTools' in this.model) || typeof this.model.bindTools !== 'function') {
        yield 'Current model does not support tool binding. Falling back to chat mode.\n\n';
        yield* this.streamChatMode(messages, systemPrompt, availableTools);
        return;
      }
      
      const modelWithTools = this.model.bindTools(tools);
      
      // ë§ˆì§€ë§‰ ì‚¬ìš©ì ë©”ì‹œì§€ ê°€ì ¸ì˜¤ê¸°
      const lastUserMessage = messages.filter(m => m.role === 'user').pop()?.content || '';
      
      yield 'ğŸ¤– Agent mode activated with tools: ' + tools.map(t => t.name).join(', ') + '\n\n';
      
      // ë„êµ¬ê°€ ë°”ì¸ë”©ëœ ëª¨ë¸ë¡œ í˜¸ì¶œ
      const response = await modelWithTools.invoke(lastUserMessage, {
        configurable: {
          apiKey: this.modelConfig.apiKey,
          model: this.modelConfig.model,
        },
      });
      
      // ì‘ë‹µì´ ë„êµ¬ í˜¸ì¶œì„ í¬í•¨í•˜ëŠ”ì§€ í™•ì¸
      if (response.tool_calls && response.tool_calls.length > 0) {
        yield 'ğŸ”§ Executing tools...\n\n';
        
        for (const toolCall of response.tool_calls) {
          try {
            yield `Calling ${toolCall.name}...\n`;
            const result = await this.executeMCPTool(toolCall.name, toolCall.args);
            yield `Tool result: ${result}\n\n`;
          } catch (error) {
            yield `Error executing ${toolCall.name}: ${error instanceof Error ? error.message : 'Unknown error'}\n\n`;
          }
        }
      }
      
      if (response.content) {
        yield response.content as string;
      }
      
    } catch (error) {
      log.error('Agent mode error', 'AIService', error instanceof Error ? error : new Error(String(error)));
      yield `Agent mode error: ${error instanceof Error ? error.message : 'Unknown error'}\n\nFalling back to chat mode...\n\n`;
      yield* this.streamChatMode(messages, systemPrompt, availableTools);
    }
  }

  // ëª¨ë¸ ì„¤ì • ì—…ë°ì´íŠ¸
  async updateModelConfig(config: Partial<ModelConfig>): Promise<void> {
    this.modelConfig = { ...this.modelConfig, ...config };
    await this.initializeModel();
  }

  // í˜„ì¬ ëª¨ë¸ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
  getModelConfig(): ModelConfig {
    return { ...this.modelConfig };
  }

  // í˜„ì¬ ì„œë¹„ìŠ¤ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
  getServiceConfig(): ServiceConfig {
    return { ...this.serviceConfig };
  }

  // í˜„ì¬ ëª¨ë¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
  getCurrentModelInfo() {
    if (!this.modelConfig.provider || !this.modelConfig.model) {
      return null;
    }
    
    return llmConfigManager.getModel(this.modelConfig.provider, this.modelConfig.model);
  }

  // ë„êµ¬ ì§€ì› ì—¬ë¶€ í™•ì¸
  supportsTools(): boolean {
    const modelInfo = this.getCurrentModelInfo();
    return modelInfo?.supportTools || false;
  }

  // ì¶”ë¡  ì§€ì› ì—¬ë¶€ í™•ì¸
  supportsReasoning(): boolean {
    const modelInfo = this.getCurrentModelInfo();
    return modelInfo?.supportReasoning || false;
  }

  // ìŠ¤íŠ¸ë¦¬ë° ì§€ì› ì—¬ë¶€ í™•ì¸
  supportsStreaming(): boolean {
    const modelInfo = this.getCurrentModelInfo();
    return modelInfo?.supportStreaming || false;
  }

  // ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° í¬ê¸° ê°€ì ¸ì˜¤ê¸°
  getContextWindow(): number {
    const modelInfo = this.getCurrentModelInfo();
    return modelInfo?.contextWindow || 0;
  }
}

let aiService: AIService | null = null;

export function getAIService(config?: Partial<ModelConfig>): AIService {
  if (!aiService || config) {
    aiService = new AIService(config);
  }
  return aiService;
}

// í¸ì˜ í•¨ìˆ˜ë“¤
export function createAIService(config: Partial<ModelConfig>): AIService {
  return new AIService(config);
}

export async function initializeAIService(config?: Partial<ModelConfig>): Promise<AIService> {
  const service = getAIService(config);
  await service.initializeModel();
  return service;
}

// ì„œë¹„ìŠ¤ IDë¡œ AI ì„œë¹„ìŠ¤ ìƒì„±
export function createAIServiceFromServiceId(serviceId: string): AIService {
  return new AIService({ serviceId });
}

// ëª¨ë¸ ì¶”ì²œì„ í†µí•œ AI ì„œë¹„ìŠ¤ ìƒì„±
export function createRecommendedAIService(requirements: {
  needsTools?: boolean;
  needsReasoning?: boolean;
  maxCost?: number;
  preferSpeed?: boolean;
  contextWindow?: number;
}): AIService {
  const recommendation = llmConfigManager.recommendModel(requirements);
  
  if (!recommendation) {
    throw new Error('No suitable model found for the given requirements');
  }

  return new AIService({
    provider: recommendation.providerId,
    model: recommendation.modelId,
  });
}

// ì‚¬ìš© ê°€ëŠ¥í•œ ì„œë¹„ìŠ¤ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
export function getAvailableServices(): Record<string, ServiceConfig> {
  return llmConfigManager.getServiceConfigs();
}

// ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
export function getAvailableModels() {
  return llmConfigManager.getAllModels();
}
